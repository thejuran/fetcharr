---
phase: 13-ci-search-diagnostics
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - fetcharr/clients/sonarr.py
  - fetcharr/startup.py
  - fetcharr/search/engine.py
  - tests/test_startup.py
  - tests/test_search.py
autonomous: true
requirements:
  - SRCH-15
  - SRCH-16

must_haves:
  truths:
    - "Fetcharr logs the detected Sonarr API version (v3 or v4) at startup"
    - "If Sonarr version detection fails, a warning is logged and v3 is assumed"
    - "Each search cycle logs total items fetched, items searched, items skipped, and cycle duration"
    - "Diagnostic logging is consistent across Radarr and Sonarr for both missing and cutoff queues"
    - "All diagnostic logs are at INFO level"
  artifacts:
    - path: "fetcharr/clients/sonarr.py"
      provides: "Sonarr API version detection method"
      contains: "detect_api_version"
    - path: "fetcharr/startup.py"
      provides: "Version detection called during startup"
      contains: "detect_api_version"
    - path: "fetcharr/search/engine.py"
      provides: "Per-cycle diagnostic summary logging"
      contains: "cycle completed"
    - path: "tests/test_startup.py"
      provides: "Tests for version detection at startup"
      contains: "detect_api_version"
    - path: "tests/test_search.py"
      provides: "Tests for diagnostic cycle logging"
      contains: "cycle completed"
  key_links:
    - from: "fetcharr/startup.py"
      to: "fetcharr/clients/sonarr.py"
      via: "calls detect_api_version during validate_connections"
      pattern: "detect_api_version"
    - from: "fetcharr/search/engine.py"
      to: "loguru"
      via: "logger.info with cycle summary"
      pattern: "logger\\.info.*cycle completed"
---

<objective>
Add Sonarr API version detection at startup and per-cycle diagnostic summary logging for troubleshooting.

Purpose: Users need to see which Sonarr API version was detected (for troubleshooting v3 vs v4 issues) and per-cycle summaries (items fetched, searched, skipped, duration) so they can diagnose pageSize truncation or unexpected behavior in large libraries.
Output: Version detection in `sonarr.py`/`startup.py`, diagnostic logging in `engine.py`, and tests for both.
</objective>

<execution_context>
@/Users/julianamacbook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/julianamacbook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@fetcharr/clients/sonarr.py
@fetcharr/clients/base.py
@fetcharr/startup.py
@fetcharr/search/engine.py
@tests/test_startup.py
@tests/test_search.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Sonarr API version detection</name>
  <files>fetcharr/clients/sonarr.py, fetcharr/startup.py, tests/test_startup.py</files>
  <action>
**SonarrClient version detection (`fetcharr/clients/sonarr.py`):**

Add an async method `detect_api_version(self) -> str` to `SonarrClient`:
- Call `GET /api/v3/system/status` (same endpoint used by `validate_connection`)
- Parse the response JSON and extract the `version` field (e.g., "3.0.10.1567" or "4.0.1.929")
- Determine v3 vs v4 by checking if the major version starts with "4" (`version.startswith("4")`)
- Return `"v4"` if major version is 4+, otherwise `"v3"`
- On any exception (httpx.HTTPError, pydantic.ValidationError, KeyError, etc.): log a warning "Sonarr: API version detection failed -- assuming v3" and return `"v3"`
- This is purely informational -- no behavior changes based on result
- Use `self._client.get()` directly (no retry needed, validate_connection already confirmed connectivity)

**Startup integration (`fetcharr/startup.py`):**

In `validate_connections()`, after the Sonarr `validate_connection()` call succeeds (returns True):
- Call `api_version = await client.detect_api_version()`
- Log at INFO level: `"Sonarr: Detected API {version}"` with version being the returned string (e.g., "v3" or "v4")
- If `validate_connection()` returned False, skip version detection (no point probing a broken connection)

**Tests (`tests/test_startup.py`):**

Add tests for version detection:
1. `test_sonarr_version_detection_v3`: Mock Sonarr client's `detect_api_version` to return "v3". Verify startup logs contain "Detected API v3".
2. `test_sonarr_version_detection_v4`: Mock Sonarr client's `detect_api_version` to return "v4". Verify startup logs contain "Detected API v4".
3. `test_sonarr_version_detection_failure`: Mock `detect_api_version` to raise httpx.ConnectError. Verify it returns "v3" (fallback) and logs a warning.

Also add a unit test directly on `SonarrClient.detect_api_version`:
4. `test_detect_api_version_parses_v3`: Use MockTransport returning `{"version": "3.0.10.1567"}`. Assert returns "v3".
5. `test_detect_api_version_parses_v4`: Use MockTransport returning `{"version": "4.0.1.929"}`. Assert returns "v4".
6. `test_detect_api_version_handles_error`: Use MockTransport that raises. Assert returns "v3".

Follow existing test patterns: `pytest-asyncio` with `asyncio_mode=auto`, `MockTransport` for client-level tests, `AsyncMock` for integration-level tests.
  </action>
  <verify>
    <automated>cd /Users/julianamacbook/fetcharr && uv run pytest tests/test_startup.py -x -q -k "version"</automated>
  </verify>
  <done>SonarrClient has detect_api_version method. Startup calls it after successful connection validation and logs the result at INFO. Failure falls back to v3 with a warning. All version detection tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Add per-cycle diagnostic summary logging</name>
  <files>fetcharr/search/engine.py</files>
  <action>
Add diagnostic summary logging to both `run_radarr_cycle` and `run_sonarr_cycle` in `fetcharr/search/engine.py`.

**For `run_radarr_cycle`:**

1. Record cycle start time at the top of the function: `cycle_start = time.monotonic()` (import `time` at module level).
2. Track counters: `searched_count = 0`, `skipped_count = 0` initialized before the missing queue processing.
3. In the missing queue loop: increment `searched_count` on successful search, increment `skipped_count` on exception.
4. In the cutoff queue loop: same counter increments (continue using the same counters across both queues).
5. After both queues complete (before updating `last_run`), compute `elapsed = time.monotonic() - cycle_start` and log at INFO level:

```
logger.info(
    "Radarr: Cycle completed in {elapsed:.1f}s -- {fetched} fetched, {searched} searched, {skipped} skipped",
    elapsed=elapsed,
    fetched=state["radarr"]["missing_count"] + state["radarr"]["cutoff_count"],
    searched=searched_count,
    skipped=skipped_count,
)
```

The `fetched` count uses the raw cached counts (set earlier in the function from `len(missing)` + `len(cutoff)`) which represents total items before filtering. This is the count users need to detect pageSize truncation on large libraries.

**For `run_sonarr_cycle`:**

Identical pattern:
1. `cycle_start = time.monotonic()` at top
2. `searched_count` and `skipped_count` counters
3. Increment in both missing and cutoff queue loops
4. Summary log after both queues:

```
logger.info(
    "Sonarr: Cycle completed in {elapsed:.1f}s -- {fetched} fetched, {searched} searched, {skipped} skipped",
    elapsed=elapsed,
    fetched=state["sonarr"]["missing_count"] + state["sonarr"]["cutoff_count"],
    searched=searched_count,
    skipped=skipped_count,
)
```

**Important:** All diagnostic logs at INFO level (per CONTEXT.md decision -- always visible, no opt-in needed). Both apps get the same diagnostic treatment for consistency. The "fetched" number is the total raw item count from the API before any filtering.

Also add `import time` at the top of engine.py alongside existing imports.
  </action>
  <verify>
    <automated>cd /Users/julianamacbook/fetcharr && uv run pytest tests/test_search.py -x -q && uv run ruff check fetcharr/search/engine.py</automated>
  </verify>
  <done>Both run_radarr_cycle and run_sonarr_cycle log a summary line at INFO with cycle duration, items fetched (raw count), items searched, and items skipped. Import of time module added.</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for diagnostic cycle logging</name>
  <files>tests/test_search.py</files>
  <action>
Add tests to `tests/test_search.py` that verify the diagnostic summary logging in cycle functions.

**Test 1: `test_radarr_cycle_logs_diagnostic_summary`**
- Use existing test patterns: `AsyncMock` for RadarrClient, `make_settings()` + `_default_state()` from conftest
- Mock `get_wanted_missing` to return 3 monitored movies, `get_wanted_cutoff` to return 2 monitored movies
- Mock `search_movies` to succeed
- Mock `insert_search_entry` (patch `fetcharr.search.engine.insert_search_entry`)
- Run `run_radarr_cycle`
- Use `caplog` or patch `loguru.logger` to capture log output
- Assert log contains "Radarr: Cycle completed in" and "5 fetched" (3+2) and "searched" and "skipped"

**Test 2: `test_sonarr_cycle_logs_diagnostic_summary`**
- Same pattern but for `run_sonarr_cycle`
- Mock `get_wanted_missing` to return episodes (with series data for dedup), `get_wanted_cutoff` to return episodes
- Assert log contains "Sonarr: Cycle completed in" with fetched/searched/skipped counts

**Test 3: `test_radarr_cycle_counts_skipped_on_search_failure`**
- Mock `search_movies` to raise an exception for one movie
- Assert the summary log shows the correct skipped count (1 skipped)

For capturing loguru output in tests, use the `capfd` fixture (loguru writes to stderr by default) or add a temporary loguru sink that captures messages. The existing test patterns in the project may use `capfd` -- check `tests/test_search.py` for the approach. If no log capture pattern exists, add a simple one: `loguru.logger.add(sink, format="{message}")` in the test and remove after.

Follow existing test patterns in `tests/test_search.py` and `tests/conftest.py`.
  </action>
  <verify>
    <automated>cd /Users/julianamacbook/fetcharr && uv run pytest tests/test_search.py -x -q -k "diagnostic"</automated>
  </verify>
  <done>Tests verify that both Radarr and Sonarr cycles produce diagnostic summary logs with correct fetched/searched/skipped counts. Skipped count increments correctly when individual searches fail.</done>
</task>

</tasks>

<verification>
- `fetcharr/clients/sonarr.py` has `detect_api_version` method that returns "v3" or "v4"
- `fetcharr/startup.py` calls version detection after successful Sonarr connection validation
- `fetcharr/search/engine.py` logs cycle summary (duration, fetched, searched, skipped) for both apps
- All diagnostic logs at INFO level
- `uv run pytest tests/ -x -q` passes with all new tests
- `uv run ruff check fetcharr/ tests/` passes
</verification>

<success_criteria>
Sonarr API version is detected and logged at startup. Every search cycle produces a diagnostic summary line showing total items fetched (for pageSize truncation diagnosis), items searched, items skipped, and cycle duration. All at INFO level, consistent across Radarr and Sonarr.
</success_criteria>

<output>
After completion, create `.planning/phases/13-ci-search-diagnostics/13-02-SUMMARY.md`
</output>
